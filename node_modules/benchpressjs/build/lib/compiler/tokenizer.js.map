{"version":3,"sources":["../../../lib/compiler/tokenizer.js"],"names":["tokens","first","require","Text","tokenizer","input","topLevelTokens","Object","keys","map","key","filter","token","priority","sort","a","b","length","output","cursor","lastBreak","slice","found","Tok","matches","patterns","pattern","match","text","push","escapedText","opens","closes","forEach","tokenType","startsWith","remove","Set","closeSubject","expectedSubjects","index","subject","path","test","raw","expectedSubject","pop","add","i","tok","m","has","module","exports"],"mappings":"AAAA;;AAEA,MAAM,EAAEA,MAAF,EAAUC,KAAV,KAAoBC,QAAQ,UAAR,CAA1B;;AAEA,MAAM,EAAEC,IAAF,KAAWH,MAAjB;;AAEA;;;;;AAKA,SAASI,SAAT,CAAmBC,KAAnB,EAA0B;AACxB,QAAMC,iBAAiBC,OAAOC,IAAP,CAAYR,MAAZ,EACpBS,GADoB,CAChBC,OAAOV,OAAOU,GAAP,CADS,EAEpBC,MAFoB,CAEbC,SAASA,MAAMC,QAAN,GAAiB,CAFb,EAGpBC,IAHoB,CAGf,CAACC,CAAD,EAAIC,CAAJ,KAAUD,EAAEF,QAAF,GAAaG,EAAEH,QAHV,CAAvB;;AAKA,QAAMI,SAASZ,MAAMY,MAArB;;AAEA,QAAMC,SAAS,EAAf;;AAEA,MAAIC,SAAS,CAAb;AACA,MAAIC,YAAY,CAAhB;;AAEA,SAAOD,SAASF,MAAhB,EAAwB;AACtB,UAAMI,QAAQhB,MAAMgB,KAAN,CAAYF,MAAZ,CAAd;AACA,UAAMG,QAAQrB,MAAMK,cAAN,EAAuBiB,GAAD,IAAS;AAC3C,UAAIC,OAAJ;AACA,UAAID,IAAIE,QAAR,EAAkB;AAChBD,kBAAUvB,MAAMsB,IAAIE,QAAV,EAAoBC,WAAWL,MAAMM,KAAN,CAAa,OAAMD,OAAQ,GAA3B,CAA/B,CAAV;AACD,OAFD,MAEO;AACLF,kBAAUH,MAAMM,KAAN,CAAa,OAAMJ,IAAIG,OAAQ,GAA/B,CAAV;AACD;AACD,aAAOF,WAAW,CAACD,GAAD,EAAMC,OAAN,CAAlB;AACD,KARa,CAAd;;AAUA,QAAIF,SAASjB,MAAMc,SAAS,CAAf,MAAsB,IAAnC,EAAyC;AACvC,YAAMS,OAAOvB,MAAMgB,KAAN,CAAYD,SAAZ,EAAuBD,SAAS,CAAhC,CAAb;AACA,UAAIS,IAAJ,EAAU;AACRV,eAAOW,IAAP,CAAY,IAAI1B,IAAJ,CAASyB,IAAT,CAAZ;AACD;;AAED,YAAME,cAAcR,MAAM,CAAN,EAAS,CAAT,CAApB;AACAJ,aAAOW,IAAP,CAAY,IAAI1B,IAAJ,CAAS2B,WAAT,CAAZ;;AAEAX,gBAAUW,YAAYb,MAAtB;AACAG,kBAAYD,MAAZ;AACD,KAXD,MAWO,IAAIG,KAAJ,EAAW;AAChB,YAAM,CAACC,GAAD,EAAMC,OAAN,IAAiBF,KAAvB;;AAEA,YAAMM,OAAOvB,MAAMgB,KAAN,CAAYD,SAAZ,EAAuBD,MAAvB,CAAb;AACA,UAAIS,IAAJ,EAAU;AACRV,eAAOW,IAAP,CAAY,IAAI1B,IAAJ,CAASyB,IAAT,CAAZ;AACD;AACDV,aAAOW,IAAP,CAAY,IAAIN,GAAJ,CAAQ,GAAGC,OAAX,CAAZ;;AAEAL,gBAAUK,QAAQ,CAAR,EAAWP,MAArB;AACAG,kBAAYD,MAAZ;AACD,KAXM,MAWA;AACLA,gBAAU,CAAV;AACD;AACF;AACD,QAAMS,OAAOvB,MAAMgB,KAAN,CAAYD,SAAZ,EAAuBD,MAAvB,CAAb;AACA,MAAIS,IAAJ,EAAU;AACRV,WAAOW,IAAP,CAAY,IAAI1B,IAAJ,CAASyB,IAAT,CAAZ;AACD;;AAED,QAAMG,QAAQ,EAAd;AACA,QAAMC,SAAS,EAAf;AACAd,SAAOe,OAAP,CAAgBrB,KAAD,IAAW;AACxB,QAAIA,MAAMsB,SAAN,CAAgBC,UAAhB,CAA2B,MAA3B,CAAJ,EAAwC;AACtCJ,YAAMF,IAAN,CAAWjB,KAAX;AACD,KAFD,MAEO,IAAIA,MAAMsB,SAAN,KAAoB,OAAxB,EAAiC;AACtCF,aAAOH,IAAP,CAAYjB,KAAZ;AACD;AACF,GAND;;AAQA;AACA;AACA,MAAIoB,OAAOf,MAAP,GAAgBc,MAAMd,MAA1B,EAAkC;AAChC,UAAMmB,SAAS,IAAIC,GAAJ,EAAf;AACA,UAAMC,eAAe,4BAArB;;AAEA,UAAMC,mBAAmB,EAAzB;AACA;AACArB,WAAOe,OAAP,CAAe,CAACrB,KAAD,EAAQ4B,KAAR,KAAkB;AAC/B,UAAI5B,MAAMsB,SAAN,CAAgBC,UAAhB,CAA2B,MAA3B,CAAJ,EAAwC;AACtCI,yBAAiBV,IAAjB,CACGjB,MAAM6B,OAAN,IAAiB7B,MAAM6B,OAAN,CAAcC,IAAhC,IACC9B,MAAM+B,IAAN,KAAe/B,MAAM+B,IAAN,CAAWC,GAAX,IAAkBhC,MAAM+B,IAAN,CAAWD,IAA5C,CAFH;AAID,OALD,MAKO,IAAI9B,MAAMsB,SAAN,KAAoB,OAAxB,EAAiC;AACtC,cAAMW,kBAAkBN,iBAAiBA,iBAAiBtB,MAAjB,GAA0B,CAA3C,CAAxB;AACAsB,yBAAiBO,GAAjB;;AAEA,YAAI,CAACD,eAAL,EAAsB;AACpBT,iBAAOW,GAAP,CAAWnC,KAAX;AACD,SAFD,MAEO;AACL,gBAAMY,UAAUZ,MAAMgC,GAAN,CAAUjB,KAAV,CAAgBW,YAAhB,CAAhB;AACA,cAAId,WAAWA,QAAQ,CAAR,MAAeqB,eAA9B,EAA+C;AAC7CT,mBAAOW,GAAP,CAAWnC,KAAX;AACD,WAFD,MAEO;AACL;AACA;AACA,iBAAK,IAAIoC,IAAIR,QAAQ,CAArB,EAAwBQ,IAAI9B,OAAOD,MAAnC,EAA2C+B,KAAK,CAAhD,EAAmD;AACjD,oBAAMC,MAAM/B,OAAO8B,CAAP,CAAZ;AACA,kBAAIC,IAAIf,SAAJ,CAAcC,UAAd,CAAyB,MAAzB,CAAJ,EAAsC;AACpC;AACD;AACD,kBAAIc,IAAIf,SAAJ,KAAkB,OAAtB,EAA+B;AAC7B,sBAAMgB,IAAID,IAAIL,GAAJ,CAAQjB,KAAR,CAAcW,YAAd,CAAV;AACA,oBAAIY,KAAKA,EAAE,CAAF,MAASL,eAAlB,EAAmC;AACjC;AACAT,yBAAOW,GAAP,CAAWnC,KAAX;AACA;AACD;AACF;AACF;AACF;AACF;AACF;AACF,KApCD;;AAsCA,WAAOM,OAAOP,MAAP,CAAcC,SAAS,CAACwB,OAAOe,GAAP,CAAWvC,KAAX,CAAxB,CAAP;AACD;;AAED,SAAOM,MAAP;AACD;;AAEDkC,OAAOC,OAAP,GAAiBjD,SAAjB","file":"tokenizer.js","sourcesContent":["'use strict';\n\nconst { tokens, first } = require('./tokens');\n\nconst { Text } = tokens;\n\n/**\n * Generate an array of tokens describing the template\n * @param {string} input\n * @return {Token[]}\n */\nfunction tokenizer(input) {\n  const topLevelTokens = Object.keys(tokens)\n    .map(key => tokens[key])\n    .filter(token => token.priority > 0)\n    .sort((a, b) => a.priority - b.priority);\n\n  const length = input.length;\n\n  const output = [];\n\n  let cursor = 0;\n  let lastBreak = 0;\n\n  while (cursor < length) {\n    const slice = input.slice(cursor);\n    const found = first(topLevelTokens, (Tok) => {\n      let matches;\n      if (Tok.patterns) {\n        matches = first(Tok.patterns, pattern => slice.match(`^(?:${pattern})`));\n      } else {\n        matches = slice.match(`^(?:${Tok.pattern})`);\n      }\n      return matches && [Tok, matches];\n    });\n\n    if (found && input[cursor - 1] === '\\\\') {\n      const text = input.slice(lastBreak, cursor - 1);\n      if (text) {\n        output.push(new Text(text));\n      }\n\n      const escapedText = found[1][0];\n      output.push(new Text(escapedText));\n\n      cursor += escapedText.length;\n      lastBreak = cursor;\n    } else if (found) {\n      const [Tok, matches] = found;\n\n      const text = input.slice(lastBreak, cursor);\n      if (text) {\n        output.push(new Text(text));\n      }\n      output.push(new Tok(...matches));\n\n      cursor += matches[0].length;\n      lastBreak = cursor;\n    } else {\n      cursor += 1;\n    }\n  }\n  const text = input.slice(lastBreak, cursor);\n  if (text) {\n    output.push(new Text(text));\n  }\n\n  const opens = [];\n  const closes = [];\n  output.forEach((token) => {\n    if (token.tokenType.startsWith('Open')) {\n      opens.push(token);\n    } else if (token.tokenType === 'Close') {\n      closes.push(token);\n    }\n  });\n\n  // if there are more closes than opens\n  // intelligently remove extra ones\n  if (closes.length > opens.length) {\n    const remove = new Set();\n    const closeSubject = /^<!-- END[^ ]* !?(.*) -->$/;\n\n    const expectedSubjects = [];\n    // try to find a Close with no corresponding Open\n    output.forEach((token, index) => {\n      if (token.tokenType.startsWith('Open')) {\n        expectedSubjects.push(\n          (token.subject && token.subject.path) ||\n          (token.test && (token.test.raw || token.test.path))\n        );\n      } else if (token.tokenType === 'Close') {\n        const expectedSubject = expectedSubjects[expectedSubjects.length - 1];\n        expectedSubjects.pop();\n\n        if (!expectedSubject) {\n          remove.add(token);\n        } else {\n          const matches = token.raw.match(closeSubject);\n          if (matches && matches[1] !== expectedSubject) {\n            remove.add(token);\n          } else {\n            // search for a close within close proximity\n            // that has the expected subject\n            for (let i = index + 1; i < output.length; i += 1) {\n              const tok = output[i];\n              if (tok.tokenType.startsWith('Open')) {\n                break;\n              }\n              if (tok.tokenType === 'Close') {\n                const m = tok.raw.match(closeSubject);\n                if (m && m[1] === expectedSubject) {\n                  // found one ahead, so remove the current one\n                  remove.add(token);\n                  break;\n                }\n              }\n            }\n          }\n        }\n      }\n    });\n\n    return output.filter(token => !remove.has(token));\n  }\n\n  return output;\n}\n\nmodule.exports = tokenizer;\n"]}